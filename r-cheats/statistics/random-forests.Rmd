
#### Overall notes

- After each tree is built, all of the data are run down the tree, and proximities are computed for each pair of cases. [@rfWorkings]
    - If two cases occupy the same terminal node, their proximity is increased by one.
    - At the end of the run, the proximities are normalized by dividing by the number of trees.
    - Proximities are used in replacing missing data, locating outliers, and producing illuminating low-dimensional views of the data.

- to compare random forests, perform wilcoxon test on mean differences of absolute errors. [@esl2]

In case of level errors like:

```
Error in randomForest.default(m, y, ...) :
  New factor levels in xtest not present in x
```

1. It can be due to more labels being available in test data.
2. The Function also do not like ordered factors. Original factors only!
3. To delve deeper into the actual levels in the `randomForest`-object use `rfObj$forest$xlevels`

If you are using `predict`, you might stuble upon the following error if you forgot the `keep.forest = TRUE` argument.:

```
Error in predict.randomForest(model, newdata = CV) :  No forest component in the object
```

#### Classification

- Categorical (e.g. Binary) Response
- authors suggest parameters should be (See [Notations](#supervised-notations))[@esl2rfDetails]
    - $\min(m) = 1$
    - $m = \sqrt(p)$

Example code [@rfPackage] [@sll10a]

```{r, eval=FALSE, echo=FALSE}
devtools::install_github('cran/randomForest')
```

```{r}
set.seed(71)
(iris.rf <-
	randomForest::randomForest(
		Species ~ .,
		data=iris,
		importance=TRUE,
    do.trace = 100, # adds the extra output per 100 tree iteration
		proximity=TRUE))

```

Look at variable importance:

```{r}
iris.rf %>% randomForest::importance(2) %>% round
```

- proximity: refers to a proximity matrix which is the distance $\in[0,1]$ between the observations.
    - A matrix of proximity measures among the input (based on the frequency that pairs of data points are in the same terminal nodes).
- By performing MDS on the proximity we can get a feeling for how the observations group between each-other.
- $1 - prox(n, k)$ are squared distances, hence we can do MDS on those distances to see groupings [@rfScaling]

```{r}
## Do MDS on 1 - proximity:
iris.mds <- stats::cmdscale(1 - iris.rf$proximity, eig=TRUE) # Classical (Metric) Multidimensional Scaling
op <- par(pty="s")
pairs(cbind(iris[,1:4], # Scatterplot Matrices
            iris.mds$points),
      cex=0.6,
      gap=0,
      col=c("red", "green", "blue")[as.numeric(iris$Species)],
      main="Iris Data: Predictors and MDS of Proximity Based on RandomForest")
par(op)
print(iris.mds$GOF)
```

include test data in the `randomForest` call to get the full proximity matrix between test and train data [@rfProximityForAll]

```{r}
set.seed(71)
ind <- sample(1:150,140,replace = FALSE)
train <- iris[ind,]
test <- iris[-ind,]

(iris.rf1 <- randomForest::randomForest(x = train[,1:4],
                         y = train[,5],
                         xtest = test[,1:4],
                         ytest = test[,5],
                         importance=TRUE,
                         proximity=TRUE))
dim(iris.rf1$test$prox)
```

"x" can be a matrix instead of a data frame:

```{r}
set.seed(17)
x <- matrix(runif(5e2), 100)
y <- gl(2, 50) # Generate Factor Levels: 2 levels, 50 of each level
(myrf <- randomForest::randomForest(x, y)) # parentheses leads to printing of the declaration
predict(myrf, x)

```

stratified sampling: draw 20, 30, and 20 of the species to grow each tree.

```{r}

randomForest::randomForest(Species ~ .,
             data = iris,
             sampsize=c(20, 30, 20))
```

#### Regression

- continuous response
- authors suggest parameters should be (See [Notations](#supervised-notations))[@esl2rfDetails]
    - $\min(m) = 5$
    - $m = \sqrt(p)$

Example code [@rfPackage] [@sll10b]

```{r message=FALSE}

library(MASS)
library(magrittr)
library(ggplot2)

```

```{r}

set.seed(101)
Boston %>% str # ?Boston # to see more data info. it's housing values in Boston suburbs.

train <- 1:nrow(Boston) %>% sample(300)

randomForest::randomForest(medv ~ .,
             data = Boston,
             subset = train)

nTreesToFit <- 400
maxVarsToTry <- 4 # 13
forests <-
  lapply(1:maxVarsToTry, function(varsToTry){
         randomForest::randomForest(medv ~ .,
                 data = Boston,
                 subset = train,
                 mtry = varsToTry, # number of variables tried at each split
                 importance = TRUE,
                 ntree = nTreesToFit)
  })

```

Variables of highest "Importance"

```{r}

forests[[maxVarsToTry]] %>% randomForest::importance() %>% round(2) %>% as.data.frame %>% dplyr::arrange(desc(`%IncMSE`)) # Removes row names >.<

testData <- Boston[-train,]
errors <-
  data.frame(varsPerSplit = 1:maxVarsToTry,
             oob = forests %>% sapply(function(forest){ forest$mse[nTreesToFit] }),
             test = forests %>%
               sapply(function(forest){
                 pred <- predict(forest, testData)
                 with(testData,
                      mean((medv - pred)^2))
               })) %>%
  tidyr::gather( src
                ,errors
                ,-c(varsPerSplit))

ggplot(errors, aes(x = varsPerSplit %>% as.factor, y = errors, group = src, color = src)) +
  geom_line() +
  geom_point() +
  xlab("Variables per split") +
  ylab("Mean Squared Error")

set.seed(131)
(ozone.rf <- randomForest::randomForest(Ozone ~ ., data=airquality, mtry=3,
                         importance=TRUE, na.action=na.omit))

```

Show "importance" of variables: higher value mean more important:

```{r}

ozone.rf %>% randomForest::importance() %>% round(2)

```

randomForest call with test data

```{r}

set.seed(131)
trainObservations <- seq(1:nrow(airquality)) %>% sample(133)
testData <- airquality[-trainObservations, ] %>% na.omit # no NAs in the test data!
randomForest::randomForest(Ozone ~ .,
             data=airquality,
             subset=trainObservations,
             xtest=testData %>% dplyr::select(-Ozone),
             ytest=testData %>% use_series("Ozone"), # atomic vector needed
             mtry=3,
             importance=TRUE,
             na.action=na.omit)

```

"complicated" formula:

```{r}

(swiss.rf <- randomForest::randomForest(sqrt(Fertility) ~ . - Catholic + I(Catholic < 50),
                          data=swiss))
predict(swiss.rf, swiss)

```

Test use of 53-level factor as a predictor:

```{r}

set.seed(1)
x <- data.frame(x1=gl(53, 10),
                x2=runif(530),
                y=rnorm(530))
(rf1 <- randomForest::randomForest(y ~ ., data=x, ntree=10))

```

Grow no more than 4 nodes per tree:

```{r}

randomForest::randomForest(Species ~ ., data=iris, maxnodes=4, ntree=30) %>% randomForest::treesize()

```

test proximity in regression

```{r}

(iris.rrf <-
  randomForest::randomForest(Sepal.Width ~ .,
               data=iris,
               ntree=101,
               proximity=TRUE,
               oob.prox=FALSE))
iris.rrf$proximity %>% str

```

#### Imputation

Random forests has two ways of replacing missing values.

1. fast; less performant. If the $m$-th variable is
    - continuous: compute the median of all values of this variable in class $j$. Use value to replace all missing values of the $m$-th variable in class $j$.
    - categorical: replace the most frequent non-missing value in class $j$. These replacement values are called fills.
2. computationally more expensive but has more performant (even with large amounts of missing data). Replaces missing values only in the training set.
    1. begins with a rough and inaccurate filling in of the missing values.
    2. Perform forest run and computes proximities.
    3. If $x(m,n)$ is a
        - continuous: estimate its fill as an average over the non-missing values of the $m$-th variables weighted by the proximities between the $n$-th case and the non-missing value case.
        - categorical: replace it by the most frequent non-missing value where frequency is weighted by proximity.
    4. Now iterate-construct a forest again using these newly filled in values, find new fills and iterate again. Our experience is that 4-6 iterations are enough.


`randomForest::na.roughfix` implements imputation #1.

```{r}
iris.na <- iris

set.seed(111)
for (i in 1:4) iris.na[sample(150, sample(20)), i] <- NA ## artificially drop some data values.

iris.na %>% summary
randomForest::na.roughfix(iris.na) %>% summary

randomForest::randomForest(Species ~ ., iris.na, na.action = na.omit)
randomForest::randomForest(Species ~ ., iris.na, na.action = randomForest::na.roughfix)
```

`randomForest::rfImpute` implements imputation #2.

```{r}
iris.na <- iris

set.seed(111)
for (i in 1:4) iris.na[sample(150, sample(20)), i] <- NA ## artificially drop some data values.

set.seed(222)
iris.imputed <- randomForest::rfImpute(Species ~ ., iris.na)

set.seed(333)
randomForest::randomForest(Species ~ ., iris.na, na.action = na.omit)
randomForest::randomForest(Species ~ ., iris.imputed)
```

