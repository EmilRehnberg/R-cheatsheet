
---
title: "R cheatsheet"
author: Emil Rehnberg
bibliography: refs.bib
csl: shiki.csl
output:
  pdf_document:
    highlight: zenburn
  html_document:
    toc_float: TRUE
    css: styles.css
---

```{r set-options, echo=FALSE, cache=FALSE}
options(width = 200)
```

```{r echo=FALSE, eval=FALSE}
require(rmarkdown); require(shiny)
rmdFilePath <- "r.rmd"
allexamples <- FALSE
rmarkdown::render(rmdFilePath, output_format="html_document") # "all_document"
```

```{r echo=FALSE, message=FALSE}
set.seed(308)
require(colorout)
require(dplyr)
require(tidyr)
require(magrittr)
require(ggplot2)
```

## 目的

cheatsheet for R. it's a place to dump typical annotated R-code.

## Cheats

### Helpers {.tabset .tabset-fade .tabset-pills}

#### Overview

Section for good helper functions.

#### wordArray

inspired by ruby's `%w`

```{r}
library(magrittr)
wordArray <- function(string){ string %>% strsplit("[[:space:]]+") %>% unlist }
"carat color value" %>% wordArray
```

#### dataDivision

Divide by zero with set default

```{r}
library(magrittr)
dataDivision <- function(numerator, denominator, divZero = NA){
  ( numerator / denominator ) %>%
   { ifelse(is.finite(.), ., divZero) }
}
dataDivision(10,2)
dataDivision(10,0)
```

#### getFactors

extract factor columns from data frame.

```{r}
library(magrittr)
getFactors <- function(dataFrame){
  dataFrame %>%
    names %>%
    sapply(function(colName){
             dataFrame %>%
               getElement(colName) %>%
               is.factor
    }) %>%
    dataFrame[,.]
}
exams <-
  expand.grid(res = c("Y", "N"),
              score = 1:4,
              id = LETTERS[1:5])
exams %>% head %T>% print %>% getFactors
```

#### Diagonal proportion of a matrix

returns if object is square shaped i.e. number of rows and columns are the equal

```{r}
isSquareShaped <- function(object) { equals(ncol(object), nrow(object)) }
```

calculates the diagonal proportion in a square-shaped object

```{r}
diagonalProportion <-
  function(object){
    if(object %>% isSquareShaped %>% not){
      warning("calculating the diagonal proportion only applies to squared objects")
      return(0)
    }
    divide_by(object %>% diag %>% sum,
              object %>% sum)}
(diag(3) + 0.5) %T>% print %>% diagonalProportion
```

### 雑多 {.tabset .tabset-fade .tabset-pills}

#### Overview

Section for "tricks".

#### saving files

`save` saves the R-object(s) to disc and keeps the naming

```{r}
# save(foo,file="data.Rda")
# load("data.Rda")
```

`saveRDS` saves one object and does not keep the name for the object.

```{r}
# saveRDS(foo, file="data.Rda")
# bar <- readRDS(file="data.Rda")
```

```{r}
# write
# write.table
# write.csv
```

```{r}
# library(MASS)
# mat <- matrix(1:100,nrow=20)
# write.matrix(mat,'file.prn',sep = "\t")
#
# mat2 <- as.matrix(read.table("file.prn", as.is = TRUE))
# # make mat2 a true matrix by removing the names
# mat2 <- unname(mat2)
# all.equal(mat, mat2) # [1] TRUE
```

```{r}
# dput; dget
```

```{r}
# dump; source
```

#### remove names

```{r}
require(magrittr)
dt <- data.frame(V1 = runif(100) %>% round,
                 V2 = runif(100) %>% round)
(crossMatrix <-
  ( with(dt, table(V1,V2))
    %>% as.data.frame(row.names = LETTERS[1:4])
    %>% as.matrix
    )
  )
crossMatrix %>% unname

# or NULLify either row or column names

colnames(crossMatrix) <- NULL
crossMatrix
rownames(crossMatrix) <- NULL
crossMatrix
```

#### Citation

```{r message=FALSE}
require(magrittr)
```

```{r}
citation() %>% toBibtex # defaults to R-documentation
citation("magrittr") %>% toBibtex
```

#### remove named columns
```{r message=FALSE}
require(magrittr)
```

```{r}
df <- data.frame(x=1:5, y=2:6, z=3:7, u=4:8)
df
df %>% subset(select=-c(z,u))
df[ , -which(names(df) %in% c("z","u"))] # also works but not as nice IMO
```

#### generate factor levels

```{r}
gl(2, 3, labels = c("Control", "Treatment"))
```

#### drop factor levels

Use the `droplevels` function. Returns the full argument object with pruned factor levels.

```{r}
df <-
  data.frame(letters=letters[1:5],
             numbers=seq(1:5))
levels(df$letters)
subdf <- df %>% subset(numbers <= 3)
levels(subdf$letters)
subdf$letters <- droplevels(subdf$letters)
levels(subdf$letters)
```

#### pipe anonymous function

wrap the function in paratheses.

```{r}
f1 <- gl(2,4) %>% sample
f2 <- gl(2,4) %>% sample
table(f1, f2) %>%
  (function(table){ divide_by(table %>% diag %>% sum,
                              table %>% sum) })
# or use special magrittr syntax-sugar
table(f1, f2) %>%
  { divide_by(diag(.) %>% sum,
              sum(.)) }
```

#### print declared variable

use parentheses to return / print result during variable declaration

```{r}
(a <- 1)
a
```

#### dicotomize

discretize continuous variable. (turn continuous variable discrete)

```{r}
Z <- stats::rnorm(10000)
table(cut(Z, breaks = -6:6))

breaks <- c(-3, 1, 5) # one more break than labels
labels <- c("low", "high")
U <- runif(10, min=-3, max=5)
cut(U,
    breaks = breaks,
    labels = labels,
    ordered_result = TRUE, # for ordered factors (not always well supported)
    include.lowest = TRUE)
```

#### regexes

- see `?regex` for more details on Regular Expressions
- can be used for many things. e.g. `strsplit`

```{r}
"name bad  good \t heaven" %>% strsplit("[[:space:]]+") %>% unlist
```

#### edit factors

alter factor-levels

- use a look-up table
- NOTE: use `as.character` to avoid serious bugs

```{r}
(fvar <-
  gl(n = 3, # number of levels
     k = 2, # number of replications
     labels = c("Control", "Treatment1", "Treatment2")))
factorLookUp <-
  c(
     "Treatment1" = "trmt"
    ,"Treatment2" = "trmt"
    ,"Control"    = "ctrl"
    )

# BUG!! use `as.character`
# fvar %>% factorLookUp[.] %>% as.factor

fvar %>% as.character %>% factorLookUp[.] %>% as.factor
```

#### Tables

```{r}
require(gmodels)
data(infert, package = "datasets")

CrossTable(warpbreaks$wool,
           warpbreaks$tension,
           chisq = TRUE,
           prop.t = TRUE,
           digits = 2,
           dnn = c("Wool", "Tension"))

CrossTable(infert$education, infert$induced, expected = TRUE)
CrossTable(infert$education, infert$induced, expected = TRUE, format="SAS")
CrossTable(infert$education, infert$induced, expected = TRUE, format="SPSS")
```

#### padding

padding with zeros

```{r}
sprintf("%04d", 20)
```

#### transforming factors

```{r}
require(magrittr)
levels <- 1:4
factorVariable <- factor(levels,
                         labels=levels+10)
factorVariable %>% as.integer # returns underlying levels
factorVariable %>% as.character # returns labels
```

#### dplyr

- different ways of using `dplyr::select`
- use array with `one_of` to select columns from data frame

```{r}
require(magrittr); require(dplyr)
airquality %>% head %>% select(Temp, Wind)
airquality %>% head %>% select_("Temp", "Wind")

columns <- "Temp Wind" %>% wordArray
airquality %>% head %>% select(one_of(columns))
# airquality %>% head %>% select_(one_of(columns)) # ERRORS: can not find function `one_of`
```

#### cumulatively sum within a group

Cumulativily sum the `carb` column within each `am`-group

```{r}
require(magrittr)
require(dplyr)
mtcars %>%
  filter(cyl == "6") %>%
  select(carb, gear, am) %>%
  group_by(am) %>%
  arrange(am)

.Last.value %>%
  do(cumsum(.["carb"]))
```

#### list to data.frame

given a list of data frames. use `Reduce` to bind them togetherq

```{r}
library(magrittr)
library(dplyr)
c(3, 6, 12) %>%
  lapply(function(day){ airquality %>% filter(Day == day) }) %>%
  Reduce(rbind, .)
```

### Plots {.tabset .tabset-fade .tabset-pills}

#### Overview

- `ggplot2` will have precedence to base `graphics::plot`

#### Typical

```{r}
(ggplot(airquality,
        aes(x = paste(Month, Day, sep="-"), y = Wind))
  + geom_point(size = .7)
  + geom_line(size = .5, colour = "blue")
  + ylab("Wind (mph)")
  + xlab("Days")
  + ggtitle("Wind over time")
  + geom_hline(aes(yintercept = median(Wind)), colour = "red")
  + scale_x_discrete(breaks=NULL)
)
```

#### Histogram

```{r}
ggplot(airquality, aes(Wind)) + geom_histogram()
```

#### Boxplot

```{r}
require(ggplot2)
(ggplot(mpg, aes(class, hwy))
 + geom_boxplot(outlier.size = 0.5)
)
```

#### Facet grid

- use the formula to tell how and what you want to stratify the data on.
- Plot is split differently if `Species` is on the x-side
- some added smoothing
- use y and x side if you want to split on multiple categorical variables

```{r}
( ggplot(iris, aes(y = Sepal.Length,
                   x = Sepal.Width))
  + facet_grid(Species ~ .)
  + geom_smooth()
  )
```

#### Matrices {.tabset .tabset-fade .tabset-pills}

##### Overview

useful to see pairwise correlation of variables.

##### graphics::pairs

pairs with added loess smoother in lower and correlation in upper

```{r}
panel.cor <- function(x, y, digits=2, prefix="", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits=digits)[1]
    txt <- paste(prefix, txt, sep="")
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}

pairs(~Sepal.Length+Sepal.Width+Petal.Length+Petal.Width, data=iris,
      lower.panel=panel.smooth, upper.panel=panel.cor,
      pch=20, main="Iris Scatterplot Matrix")
```

##### lattice

Has options to condition the scatterplot matrix on a factor.

```{r}
require(lattice)
super.sym <- trellis.par.get("superpose.symbol")
splom(~iris[1:4],
      groups = Species,
      data = iris,
      panel = panel.superpose,
      key = list(title = "Three Varieties of Iris",
                 columns = 3,
                 points = list(pch = super.sym$pch[1:3],
                 col = super.sym$col[1:3]),
                 text = list(c("Setosa", "Versicolor", "Virginica"))))
```

##### car

The `car` package can condition the scatterplot matrix on a factor, and optionally include lowess and linear best fit lines, and boxplot, densities, or histograms in the principal diagonal, as well as rug plots in the margins of the cells.

```{r}
require(car)
scatterplot.matrix(~mpg+disp+drat+wt|cyl, data=mtcars,
  	main="Three Cylinder Options")
```

##### gclus

The `gclus` package provides options to rearrange the variables so that those with higher correlations are closer to the principal diagonal. It can also color code the cells to reflect the size of the correlations.

```{r}
require(gclus)
dta <- mtcars[c(1,3,5,6)] # get data
dta.r <- abs(cor(dta)) # get correlations
dta.col <- dmat.color(dta.r) # get colors
# reorder variables so those with highest correlation
# are closest to the diagonal
dta.o <- order.single(dta.r)
cpairs(dta, dta.o, panel.colors=dta.col, gap=.5,
main="Variables Ordered and Colored by Correlation" )
```

##### gpairs

```{r}
require(gpairs)

if (allexamples) {
  y <- data.frame(A=c(rep("red", 100), rep("blue", 100)),
                  B=c(rnorm(100),round(rnorm(100,5,1),1)),
                  C=runif(200),
                  D=c(rep("big", 150), rep("small", 50)),
                  E=rnorm(200))
  gpairs(y)
}

if (allexamples) {
  data(iris)
  gpairs(iris)
  gpairs(iris, upper.pars = list(scatter = 'stats'),
         scatter.pars = list(pch = substr(as.character(iris$Species), 1, 1),
                             col = as.numeric(iris$Species)),
         stat.pars = list(verbose = FALSE))
  gpairs(iris, lower.pars = list(scatter = 'corrgram'),
         upper.pars = list(conditional = 'boxplot', scatter = 'loess'),
         scatter.pars = list(pch = 20))
}

if (allexamples) {
  data(Leaves)
  gpairs(Leaves[1:10], lower.pars = list(scatter = 'loess'))
  gpairs(Leaves[1:10], upper.pars = list(scatter = 'stats'),
         lower.pars = list(scatter = 'corrgram'),
         stat.pars = list(verbose = FALSE), gap = 0)
  corrgram(Leaves[,-33])
}
```

##### GGally

```{r}
require(GGally)
require(dplyr)
if (allexamples){
  ds <- read.csv("for-gally.csv")
  ds$sex <- ifelse(ds$female==1, "female", "male") %>% as.factor
  ds$housing <- ifelse(ds$homeless==1, "homeless", "housed") %>% as.factor
  ggpairs(ds,
  				columns=c("i1", "cesd", "housing", "sex"), # list the factor variables first to get better boxplots
  				diag=list(continuous="density",
  									discrete="bar"),
  				axisLabels="show")
}

if (allexamples){
  data(tips, package = "reshape")
  ggpairs(tips[, 1:3])
  ggpairs(tips, 1:3, columnLabels = c("Total Bill", "Tip", "Sex"))
  ggpairs(tips, 1:3, upper = "blank")
  # Only Variable Labels on the diagonal (no axis labels)
  ggpairs(tips[, 1:3], axisLabels="internal")
  # Only Variable Labels on the outside (no axis labels)
  ggpairs(tips[, 1:3], axisLabels="none")

  ggpairs(
    tips[, c(1, 3, 4, 2)],
    upper = list(continuous = "density", combo = "box"),
    lower = list(continuous = "points", combo = "dot")
  )
}

if (allexamples){
  data(diamonds, package="ggplot2")
  diamonds.samp <- diamonds %>% sample_n(30)

  ggpairs(
    diamonds.samp,
    columns = "carat cut clarity depth" %>% wordArray,
    mapping = ggplot2::aes(color = cut),
    upper = list(continuous = wrap("density", alpha = 0.5),
                 combo = "box"),
    lower = list(continuous = wrap("points", alpha = 0.3),
                 combo = wrap("dot", alpha = 0.4)),
    title = "Diamonds"
  )
}

custom_car <- ggpairs(mtcars[, c("mpg", "wt", "cyl")], upper = "blank", title = "Custom Example")
# ggplot example taken from example(geom_text)
  plot <- ggplot2::ggplot(mtcars, ggplot2::aes(x=wt, y=mpg, label=rownames(mtcars)))
  plot <- plot +
    ggplot2::geom_text(ggplot2::aes(colour=factor(cyl)), size = 3) +
    ggplot2::scale_colour_discrete(l=40)
custom_car[1, 2] <- plot
personal_plot <- ggally_text(
  "ggpairs allows you\nto put in your\nown plot.\nLike that one.\n <---"
)
custom_car[1, 3] <- personal_plot
custom_car
```

##### ggplot2::plotmatrix

Depricated.

```{r}
# require(ggplot2)
# plotmatrix(with(iris, data.frame(Sepal.Length, Sepal.Width, Petal.Length, Petal.Width)))
```

#### ggplot2 with multiple datasets and colors

```{r}
library(dplyr)
library(magrittr)
library(ggplot2)
dpassed <- rep(c("Yes", "No"), each=1000)
dspeed <- c(rnorm(1000, 2),
           rnorm(1000, 1))
lvlData <- data.frame(passed = dpassed, speed = dspeed)
samples <- factor(c("Passed", "Total"))
# TODO: FIX ME
# (
#   ggplot(lvlData %>% filter(passed == "Yes"),
#          aes(x=speed, color="Sample", shape="Sample", linetype="Sample"))
#   + stat_ecdf(data=lvlData,
#               aes(x=speed, color="Total", shape="Total", linetype="Total"))
#   + stat_ecdf(aes(color="Passed", shape="Passed", linetype="Passed"))
#   + xlab("")
#   + ylab("inclusion")
#   + labs(title="Average days to completion")
#   + scale_color_manual(breaks=samples, values=c("green", "red"))
#   + scale_shape_manual(breaks=samples, values=c(16, 16))
#   + scale_linetype_manual(breaks=samples, values=c(1, 1))
#   + labs(color = "dataset")
#   + theme(legend.title=element_blank())
# )
```

### Tests {.tabset .tabset-fade .tabset-pills}

#### Overview

Section for test. Parametric and non-paratmetric.

#### Mann-Whitney-Wilcoxon Test

In statistics, the Mann–Whitney U test (also called the Mann–Whitney–Wilcoxon (MWW), Wilcoxon rank-sum test, or Wilcoxon–Mann–Whitney test) is a nonparametric test of the null hypothesis that two samples come from the same population against an alternative hypothesis, especially that a particular population tends to have larger values than the other.[@wikiMannWhitneyUTest]

Unlike the t-test it does not require the assumption of normal distributions. It is nearly as efficient as the t-test on normal distributions.[@wikiMannWhitneyUTest]

Use the Wilcoxon signed-rank test when samples are related / paired.

Example code [@wilcoxTest]

```{r}
require(graphics)
## One-sample test.
x <- c(1.83,  0.50,  1.62,  2.48, 1.68, 1.88, 1.55, 3.06, 1.30)
y <- c(0.878, 0.647, 0.598, 2.05, 1.06, 1.29, 1.06, 3.14, 1.29)
wilcox.test(y - x, alternative = "less",
            exact = FALSE, correct = FALSE) # H&W large sample approximation

## Two-sample test.
x <- c(0.80, 0.83, 1.89, 1.04, 1.45, 1.38, 1.91, 1.64, 0.73, 1.46)
y <- c(1.15, 0.88, 0.90, 0.74, 1.21)
wilcox.test(x, y, alternative = "greater")
wilcox.test(x, y, alternative = "greater",
            exact = FALSE, correct = FALSE) # H&W large sample approximation

wilcox.test(rnorm(10), rnorm(10, 2), conf.int = TRUE)

## Formula interface.
boxplot(Ozone ~ Month, data = airquality)
wilcox.test(Ozone ~ Month, data = airquality,
            subset = Month %in% c(5, 8))
```

#### Wilcoxon signed-rank test

The Wilcoxon signed-rank test is a non-parametric statistical hypothesis test used when comparing two related samples, matched samples, or repeated measurements on a single sample to assess whether their population mean ranks differ (i.e. it is a paired difference test). It can be used as an alternative to the paired Student's t-test, t-test for matched pairs, or the t-test for dependent samples when the population cannot be assumed to be normally distributed. [@lowry11]

Example code [@wilcoxTest], [@rTutorWilcoxonSignedRankTest]

```{r}
x <- c(1.83,  0.50,  1.62,  2.48, 1.68, 1.88, 1.55, 3.06, 1.30)
y <- c(0.878, 0.647, 0.598, 2.05, 1.06, 1.29, 1.06, 3.14, 1.29)
wilcox.test(x, y, paired = TRUE, alternative = "greater")

require(MASS)
wilcox.test(immer$Y1, immer$Y2, paired=TRUE)
```

### Supervised {.tabset .tabset-fade .tabset-pills}

#### Notations {#supervised-notations}

- $p$ number of variables / parameters to fit
- $m$ number of variable to try each split (Random Forest)

#### PLS - Partial Least Squares
```{r}
```

#### Random Forests {.tabset .tabset-fade .tabset-pills}

##### Overall notes

- After each tree is built, all of the data are run down the tree, and proximities are computed for each pair of cases. [@rfWorkings]
    - If two cases occupy the same terminal node, their proximity is increased by one.
    - At the end of the run, the proximities are normalized by dividing by the number of trees.
    - Proximities are used in replacing missing data, locating outliers, and producing illuminating low-dimensional views of the data.

- to compare random forests, perform wilcoxon test on mean differences of absolute errors. [@esl2]

In case of level errors like:

```
Error in randomForest.default(m, y, ...) :
  New factor levels in xtest not present in x
```

1. It can be due to more labels being available in test data.
2. The Function also do not like ordered factors. Original factors only!
3. To delve deeper into the actual levels in the `randomForest`-object use `rfObj$forest$xlevels`

If you are using `predict`, you might stuble upon the following error if you forgot the `keep.forest = TRUE` argument.:

```
Error in predict.randomForest(model, newdata = CV) :  No forest component in the object
```

##### Classification

- Categorical (e.g. Binary) Response
- authors suggest parameters should be (See [Notations](#supervised-notations))[@esl2rfDetails]
    - $\min(m) = 1$
    - $m = \sqrt(p)$

Example code [@rfPackage] [@sll10a]

```{r}
require(randomForest)
##data(iris)
set.seed(71)
(iris.rf <-
	randomForest(
		Species ~ .,
		data=iris,
		importance=TRUE,
    do.trace = 100, # adds the extra output per 100 tree iteration
		proximity=TRUE))
## Look at variable importance:
round(importance(iris.rf), 2)
```

- proximity: refers to a proximity matrix which is the distance $\in[0,1]$ between the observations.
    - A matrix of proximity measures among the input (based on the frequency that pairs of data points are in the same terminal nodes).
- By performing MDS on the proximity we can get a feeling for how the observations group between each-other.
- $1 - prox(n, k)$ are squared distances, hence we can do MDS on those distances to see groupings [@rfScaling]

```{r}
## Do MDS on 1 - proximity:
iris.mds <- cmdscale(1 - iris.rf$proximity, eig=TRUE) # Classical (Metric) Multidimensional Scaling
op <- par(pty="s")
pairs(cbind(iris[,1:4], # Scatterplot Matrices
            iris.mds$points),
      cex=0.6,
      gap=0,
      col=c("red", "green", "blue")[as.numeric(iris$Species)],
      main="Iris Data: Predictors and MDS of Proximity Based on RandomForest")
par(op)
print(iris.mds$GOF)
```

include test data in the `randomForest` call to get the full proximity matrix between test and train data [@rfProximityForAll]

```{r}
set.seed(71)
ind <- sample(1:150,140,replace = FALSE)
train <- iris[ind,]
test <- iris[-ind,]

(iris.rf1 <- randomForest(x = train[,1:4],
                         y = train[,5],
                         xtest = test[,1:4],
                         ytest = test[,5],
                         importance=TRUE,
                         proximity=TRUE))
dim(iris.rf1$test$prox)
```

```{r}
## "x" can be a matrix instead of a data frame:
set.seed(17)
x <- matrix(runif(5e2), 100)
y <- gl(2, 50) # Generate Factor Levels: 2 levels, 50 of each level
(myrf <- randomForest(x, y)) # parentheses leads to printing of the declaration
predict(myrf, x)

## stratified sampling: draw 20, 30, and 20 of the species to grow each tree.
##data(iris)
randomForest(Species ~ .,
             data = iris,
             sampsize=c(20, 30, 20))
```

##### Regression

- continuous response
- authors suggest parameters should be (See [Notations](#supervised-notations))[@esl2rfDetails]
    - $\min(m) = 5$
    - $m = \sqrt(p)$

Example code [@rfPackage] [@sll10b]

```{r message=FALSE}
require(MASS)
require(randomForest)
require(magrittr)
```

```{r}
set.seed(101)
# ?Boston # to see more data info. it's housing values in Boston suburbs.
Boston %>% str

train <- 1:nrow(Boston) %>% sample(300)

randomForest(medv ~ .,
             data = Boston,
             subset = train)

nTreesToFit <- 400
maxVarsToTry <- 4 # 13
forests <-
  lapply(1:maxVarsToTry, function(varsToTry){
    randomForest(medv ~ .,
                 data = Boston,
                 subset = train,
                 mtry = varsToTry, # number of variables tried at each split
                 importance = TRUE,
                 ntree = nTreesToFit)
  })

# Variables of highest "Importance"
forests[[maxVarsToTry]] %>% importance %>% round(2) %>% as.data.frame %>% arrange(desc(`%IncMSE`))

testData <- Boston[-train,]
errors <-
  data.frame(varsPerSplit = 1:maxVarsToTry,
             oob = forests %>% sapply(function(forest){ forest$mse[nTreesToFit] }),
             test = forests %>%
               sapply(function(forest){
                 pred <- predict(forest, testData)
                 with(testData,
                      mean((medv - pred)^2))
               }))
cols <- c("red", "blue")
matplot(errors$varsPerSplit,
        errors %>% subset(select = -c(varsPerSplit)),
        pch=19,
        col=cols,
        type="b",
        ylab="Mean Squared Error")
legend("topright",
       legend=c("OOB", "Test"),
       pch=19,
       col=cols)

## data(airquality)
set.seed(131)
(ozone.rf <- randomForest(Ozone ~ ., data=airquality, mtry=3,
                         importance=TRUE, na.action=na.omit))
## Show "importance" of variables: higher value mean more important:
ozone.rf %>% importance %>% round(2)

## randomForest call with test data
set.seed(131)
trainObservations <- seq(1:nrow(airquality)) %>% sample(133)
testData <- airquality[-trainObservations, ] %>% na.omit # no NAs in the test data!
randomForest(Ozone ~ .,
             data=airquality,
             subset=trainObservations,
             xtest=testData %>% dplyr::select(-Ozone),
             ytest=testData %>% use_series("Ozone"), # atomic vector needed
             mtry=3,
             importance=TRUE,
             na.action=na.omit)

## "complicated" formula:
(swiss.rf <- randomForest(sqrt(Fertility) ~ . - Catholic + I(Catholic < 50),
                          data=swiss))
predict(swiss.rf, swiss)

## Test use of 53-level factor as a predictor:
set.seed(1)
x <- data.frame(x1=gl(53, 10),
                x2=runif(530),
                y=rnorm(530))
(rf1 <- randomForest(y ~ ., data=x, ntree=10))

## Grow no more than 4 nodes per tree:
randomForest(Species ~ ., data=iris, maxnodes=4, ntree=30) %>% treesize

## test proximity in regression
(iris.rrf <-
  randomForest(Sepal.Width ~ .,
               data=iris,
               ntree=101,
               proximity=TRUE,
               oob.prox=FALSE))
iris.rrf$proximity %>% str
```

##### Imputation

Random forests has two ways of replacing missing values.

1. fast; less performant. If the $m$-th variable is
    - continuous: compute the median of all values of this variable in class $j$. Use value to replace all missing values of the $m$-th variable in class $j$.
    - categorical: replace the most frequent non-missing value in class $j$. These replacement values are called fills.
2. computationally more expensive but has more performant (even with large amounts of missing data). Replaces missing values only in the training set.
    1. begins with a rough and inaccurate filling in of the missing values.
    2. Perform forest run and computes proximities.
    3. If $x(m,n)$ is a
        - continuous: estimate its fill as an average over the non-missing values of the $m$-th variables weighted by the proximities between the $n$-th case and the non-missing value case.
        - categorical: replace it by the most frequent non-missing value where frequency is weighted by proximity.
    4. Now iterate-construct a forest again using these newly filled in values, find new fills and iterate again. Our experience is that 4-6 iterations are enough.


`randomForest::na.roughfix` implements imputation #1.

```{r}
iris.na <- iris

set.seed(111)
## artificially drop some data values.
for (i in 1:4) iris.na[sample(150, sample(20)), i] <- NA

iris.na %>% summary
na.roughfix(iris.na) %>% summary

randomForest(Species ~ ., iris.na, na.action = na.omit)
randomForest(Species ~ ., iris.na, na.action = na.roughfix)
```

`randomForest::rfImpute` implements imputation #2.

```{r}
iris.na <- iris

set.seed(111)
## artificially drop some data values.
for (i in 1:4) iris.na[sample(150, sample(20)), i] <- NA

set.seed(222)
iris.imputed <- rfImpute(Species ~ ., iris.na)

set.seed(333)
randomForest(Species ~ ., iris.na, na.action = na.omit)
randomForest(Species ~ ., iris.imputed)
```

#### Boosting { .tabset .tabset-fade .tabset-pills }

##### Regression

random forest boosting with continous response.

- authors suggest $m = \sqrt(p)$ (See [Notations](#supervised-notations))
- see latter half of `mpv Movies/statistical-learning/08-tree-based-methods/StatsLearning Lect10 R trees B 111213-IY7oWGXb77o.mp4`

```{r message=FALSE}
require(MASS)
require(magrittr)
require(gbm) # Gradient Boosted Machines
```

```{r}
set.seed(101)
# ?Boston # to see more data info. it's housing values in Boston suburbs.
Boston %>% str

train <- 1:nrow(Boston) %>% sample(300)

boost.boston <-
  gbm(medv ~ .,
      data = Boston[train,],
      distribution = "gaussian",
      n.trees = 10000,
      shrinkage = 0.01,
      interaction.depth = 4)
boost.boston %>% summary # gives variance importance plot.
# variables lstat (lower status) and rm (rooms) are impacting the most.
boost.boston %>% plot(i = "lstat") # partical dependency plot for the two most important plots.
boost.boston %>% plot(i = "rm")

n.trees <-
  seq(from = 100,
      to = 10000,
      by = 100)
testData <- Boston[-train, ]
predmat <-
  predict(boost.boston,
          newdata = testData,
          n.trees = n.trees)
predmat %>% dim
berr <-
  with(testData,
       apply((predmat - medv)^2, 2, mean))
plot(n.trees,
     berr,
     pch = 19,
     ylab = "Mean Squared Error",
     xlab = "# Trees",
     main = "Boosting Test Error")
# from the RF w/o boosting.
# run that example first if you want to compare the boost vs non-boost results.
abline(h = min(errors$test),
       col="red")
```

##### Classification

```{r message=FALSE}
require(magrittr)
require(gbm) # Gradient Boosted Machines
```

```{r}
set.seed(101)
trainObservations <- sample(1:nrow(iris), 75)
testData <- iris[-trainObservations, ]

( fit <-
	gbm(Species~.,
			data=iris[trainObservations, ],
			distribution="multinomial"))
nTrees <- 4^(1:3)

predictions <-
  predict(fit,
          newdata = testData,
          n.trees = nTrees)

( classifiationTables <-
  sapply(nTrees %>% as.character,
         function(treeSize){
           ( predictions[,,treeSize]
             %>% apply(1, which.max)
             %>% factor(labels = predictions %>% dimnames %>% .[[2]])
             %>% table(testData$Species)
            )
         }, simplify = FALSE))
# prediction accuracy
( classifiationTables
  %>% sapply(diagonalProportion)
 )

# # only thing required for one n.trees
# (
#   predictions
#   %>% apply(1, which.max)
#   %>% factor(labels = predictions %>% dimnames %>% .[[2]])
#   %>% table(testData$Species)
#  )
```

### Unsupervised { .tabset .tabset-fade .tabset-pills }

#### Overview

methods for unsupervised learning.

#### PCA/PCR

#### Random Forest

Example code [@rfPackage]

```{r}
require(randomForest)
## The `unsupervised' case:
##data(iris)
set.seed(17)
(iris.urf <- randomForest(iris %>% dplyr::select(-Species)))
MDSplot(iris.urf, iris$Species) # Species sets the colors.
```

